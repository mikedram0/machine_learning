{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "a4YiHQJuTNTp"
   },
   "source": [
    "Team: HiddenLayer\n",
    "\n",
    "Fullnames: Γιάνναρος Χρήστος, Δραμιτινός Μηχάλης και Μπίνας Βαλαβάνης Βασίλης."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "x8fcqxE2DrWQ",
    "outputId": "ffbea398-2880-43ab-d28a-7ed04c1a33a4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mounted at /content/drive\n"
     ]
    }
   ],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "gH-xLXCIiW3g",
    "outputId": "1c937e83-4bad-4d37-bb9e-4a5471622b5b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TensorFlow version found: 2.4.1\n"
     ]
    }
   ],
   "source": [
    "try :\n",
    "  # In Google Colab\n",
    "  %tensorflow_version 2.x\n",
    "except :\n",
    "  pass\n",
    "\n",
    "# Import Tensorflow\n",
    "import tensorflow as tf\n",
    "\n",
    "# Assure TensorFlow version >= 2 is installed. Otherwise install TensorFlow 2.0 version.\n",
    "try :\n",
    "  assert tf.__version__ >= \"2.0\"\n",
    "  print (\"TensorFlow version found:\", tf.__version__)\n",
    "except :\n",
    "  !pip install tensorflow==2.4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "BPv1iSjWSgwu",
    "outputId": "7a99be6b-c3ae-4b39-ce60-d2df4ccaf6ac"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: Music21 in /usr/local/lib/python3.7/dist-packages (5.5.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install Music21\n",
    "\n",
    "# Import needed modules\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import os, io, requests\n",
    "from music21 import converter, instrument, note, chord"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "N4O1RpdsSGab"
   },
   "outputs": [],
   "source": [
    "path = \"/content/drive/MyDrive/Colab Notebooks/clean_midi/clean_midi\" #path to clean_midi folder that is inside clean_midi."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "RBr0-UniBMZx"
   },
   "outputs": [],
   "source": [
    "path1 =\"/content/drive/MyDrive/midi\" #path to midi."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0000CyTEi5Vg"
   },
   "outputs": [],
   "source": [
    "artists = os.listdir(path) #a list of every artist \n",
    "songs = []\n",
    "number_of_songs_per_artist = []     # a loop that gives as a list with every song \n",
    "for i in range(len(artists)):\n",
    "  s = 0\n",
    "  if os.path.isdir(os.path.join(path, artists[i])):\n",
    "    for j in  os.listdir(os.path.join(path, artists[i])):\n",
    "      s+=1\n",
    "      songs.append(j)\n",
    "    number_of_songs_per_artist.append(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "M8lRTwmMBSps"
   },
   "outputs": [],
   "source": [
    "songs1 = []     #a loop that gives us the name of the songs of the midi folder\n",
    "for j in  os.listdir(os.path.join(path1)):\n",
    "  songs1.append(j)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Nj7b5zT9WiZ5"
   },
   "outputs": [],
   "source": [
    "len(songs1) #number of songs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "RBG0Xu74mOG4"
   },
   "outputs": [],
   "source": [
    "songs1.sort() #sorting the names of the songs alphabetically"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Qg0WCeKrWgMk"
   },
   "outputs": [],
   "source": [
    "for name in songs1: #names of all the songs\n",
    "  print(name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "QdfLKUPNs8Z8"
   },
   "outputs": [],
   "source": [
    "notes_per_artist = []                 #the loop below gives us an array with every note from all the songs of each artist \n",
    "counter = 0                           \n",
    "counter_2 = 0\n",
    "counter_3 = 0\n",
    "number_of_artists = artists[:15]\n",
    "\n",
    "for artist in number_of_artists:\n",
    "  os.chdir(os.path.join(path, artist))\n",
    "  counter_3 = counter_2\n",
    "  notes_1 = []\n",
    "  for i in range(number_of_songs_per_artist[counter]):\n",
    "    counter_2 += 1\n",
    "    if counter_2 in [21,79,256]:  #these are the potitions of the songs that can't be opened because of errors\n",
    "      pass\n",
    "    else: \n",
    "      midi = converter.parse(songs[counter_3 + i])\n",
    "      notes_1_to_parse = None\n",
    "      parts = instrument.partitionByInstrument(midi)\n",
    "      if parts: # file has instrument parts\n",
    "        notes_1_to_parse = parts.parts[0].recurse()\n",
    "      else: # file has notes in a flat structure\n",
    "          notes_1_to_parse = midi.flat.notes\n",
    "      for element in notes_1_to_parse:\n",
    "          if isinstance(element, note.Note):\n",
    "              notes_1.append(str(element.pitch))\n",
    "          elif isinstance(element, chord.Chord):\n",
    "              notes_1.append('.'.join(str(n) for n in element.normalOrder))\n",
    "  notes_per_artist.append(notes_1)\n",
    "  counter += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "a4GJxXT7BYY0"
   },
   "outputs": [],
   "source": [
    "#the loop bellow gives us an array with every note from all the songs of each artist \n",
    "\n",
    "os.chdir(os.path.join(path1))\n",
    "notes = []\n",
    "for i in range(len(songs1)):\n",
    "  notes1 = []\n",
    "  midi = converter.parse(songs1[i])\n",
    "  notes_to_parse = None\n",
    "  parts = instrument.partitionByInstrument(midi)\n",
    "  if parts: # file has instrument parts\n",
    "      notes_to_parse = parts.parts[0].recurse()\n",
    "  else: # file has notes in a flat structure\n",
    "      notes_to_parse = midi.flat.notes\n",
    "  for element in notes_to_parse:\n",
    "      if isinstance(element, note.Note):\n",
    "          notes1.append(str(element.pitch))\n",
    "      elif isinstance(element, chord.Chord):\n",
    "          notes1.append('.'.join(str(n) for n in element.normalOrder))\n",
    "  notes.append(notes1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ihf_fOHAcfME"
   },
   "source": [
    "Bellow we have the names of the songs used from clean_midi with their artists:\n",
    "\n",
    "Local H:\n",
    "\n",
    "Back in the Day.mid\n",
    "Bound for the Floor.mid\n",
    "Bag of Hammers.mid\n",
    "Bound for the Floor.1.mid\n",
    "\n",
    "Little Texas:\n",
    "\n",
    "God Bless Texas.mid\n",
    "\n",
    "Loesser:\n",
    "\n",
    "Never Will I Marry.mid\n",
    "Don't Cry.mid\n",
    "The Most Happy Fella.mid\n",
    "\n",
    "Loft:\n",
    "\n",
    "Love Is Magic.mid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "nSZopeEOFh2L"
   },
   "outputs": [],
   "source": [
    "notes_artist = notes_per_artist[0]\n",
    "N = 4 #number of artists that we will take notes from\n",
    "\n",
    "for i in range(1,N):\n",
    "  notes_artist = np.append(notes_artist, notes_per_artist[i])  #all the notes from the songs of the first N artists \n",
    "\n",
    "#Now we will create another dataset for our transfer model\n",
    "notes_artist1 = notes[0]\n",
    "\n",
    "N = 19 #number of artists that we will take notes from\n",
    "\n",
    "for i in range(1,N):\n",
    "  if i not in [10,12]:\n",
    "    notes_artist1 = np.append(notes_artist1, notes[i])  #all the notes from the all the songs except 2\n",
    "\n",
    "notes_artist2 = np.append(notes[10], notes[12]) \n",
    "\n",
    "# get all pitch names\n",
    "pitchnames = sorted(set(item for item in notes_artist))\n",
    "# create a dictionary to map pitches to integers\n",
    "note_to_int = dict((note, number) for number, note in enumerate(pitchnames))\n",
    "\n",
    "#same thing for the dataset of the transfer model\n",
    "# get all pitch names\n",
    "pitchnames1 = sorted(set(item for item in notes_artist1))\n",
    "# create a dictionary to map pitches to integers\n",
    "note_to_int1 = dict((note, number) for number, note in enumerate(pitchnames1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "zrsr3JEigkBh"
   },
   "outputs": [],
   "source": [
    "# convert an array of values into a dataset matrix\n",
    "def create_dataset(dataset, look_back = 1) :\n",
    "  dataX, dataY = [], []\n",
    "  for i in range( len(dataset) - look_back ) :\n",
    "    a = dataset[i : i + look_back]\n",
    "    dataX.append(a)\n",
    "    dataY.append(dataset[i + look_back])\n",
    "  return np.array(dataX), np.array(dataY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "kQEmjododQ8V"
   },
   "outputs": [],
   "source": [
    "window = 90\n",
    "\n",
    "# Training set :\n",
    "sequence_in = notes_artist[:]\n",
    "train = [note_to_int[char] for char in sequence_in]\n",
    "\n",
    "sequence_in = notes_artist1[:]\n",
    "train1 = [note_to_int1[char] for char in sequence_in]\n",
    "\n",
    "notes_for_the_final_product = [note_to_int1[char] for char in notes_artist2[:]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "XqlOHNLYfDUn"
   },
   "outputs": [],
   "source": [
    "train_x , train_y = create_dataset(train, window)\n",
    "train_x1 , train_y1 = create_dataset(train1, window)\n",
    "\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "\n",
    "# Convert labels to one-hot vector.\n",
    "train_y_c = to_categorical(train_y)\n",
    "train_y_c1 = to_categorical(train_y1)\n",
    "number_of_output = train_y_c.shape[1]\n",
    "number_of_output1 = train_y_c1.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "RZnESNYNh285"
   },
   "outputs": [],
   "source": [
    "# Convert to LSTM friendly format \n",
    "x_train_lstm = np.reshape(train_x, (-1, window, 1))\n",
    "x_train_lstm1 = np.reshape(train_x1, (-1, window, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "r4HVQs0KfvLq",
    "outputId": "d934baec-6fec-4d12-941c-412b2f225105"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm (LSTM)                  (None, 256)               264192    \n",
      "_________________________________________________________________\n",
      "dropout (Dropout)            (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 256)               65792     \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 126)               32382     \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 200)               25400     \n",
      "_________________________________________________________________\n",
      "activation (Activation)      (None, 200)               0         \n",
      "=================================================================\n",
      "Total params: 387,766\n",
      "Trainable params: 387,766\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/50\n",
      "322/322 [==============================] - 92s 279ms/step - loss: 3.5491\n",
      "Epoch 2/50\n",
      "322/322 [==============================] - 88s 274ms/step - loss: 2.3094\n",
      "Epoch 3/50\n",
      "322/322 [==============================] - 89s 276ms/step - loss: 2.0072\n",
      "Epoch 4/50\n",
      "322/322 [==============================] - 89s 275ms/step - loss: 1.8712\n",
      "Epoch 5/50\n",
      "322/322 [==============================] - 89s 276ms/step - loss: 1.8046\n",
      "Epoch 6/50\n",
      "322/322 [==============================] - 88s 274ms/step - loss: 1.6884\n",
      "Epoch 7/50\n",
      "322/322 [==============================] - 90s 278ms/step - loss: 1.5977\n",
      "Epoch 8/50\n",
      "322/322 [==============================] - 89s 277ms/step - loss: 1.5304\n",
      "Epoch 9/50\n",
      "322/322 [==============================] - 89s 275ms/step - loss: 1.5082\n",
      "Epoch 10/50\n",
      "322/322 [==============================] - 89s 276ms/step - loss: 1.4393\n",
      "Epoch 11/50\n",
      "322/322 [==============================] - 89s 277ms/step - loss: 1.3666\n",
      "Epoch 12/50\n",
      "322/322 [==============================] - 89s 276ms/step - loss: 1.3121\n",
      "Epoch 13/50\n",
      "322/322 [==============================] - 89s 276ms/step - loss: 1.2380\n",
      "Epoch 14/50\n",
      "322/322 [==============================] - 89s 277ms/step - loss: 1.1929\n",
      "Epoch 15/50\n",
      "322/322 [==============================] - 89s 276ms/step - loss: 1.1851\n",
      "Epoch 16/50\n",
      "322/322 [==============================] - 88s 274ms/step - loss: 1.1074\n",
      "Epoch 17/50\n",
      "322/322 [==============================] - 89s 277ms/step - loss: 1.0794\n",
      "Epoch 18/50\n",
      "322/322 [==============================] - 89s 278ms/step - loss: 1.0866\n",
      "Epoch 19/50\n",
      "322/322 [==============================] - 89s 277ms/step - loss: 1.0318\n",
      "Epoch 20/50\n",
      "322/322 [==============================] - 89s 277ms/step - loss: 1.0066\n",
      "Epoch 21/50\n",
      "322/322 [==============================] - 89s 277ms/step - loss: 0.9806\n",
      "Epoch 22/50\n",
      "322/322 [==============================] - 89s 276ms/step - loss: 0.9437\n",
      "Epoch 23/50\n",
      "322/322 [==============================] - 89s 275ms/step - loss: 0.9231\n",
      "Epoch 24/50\n",
      "322/322 [==============================] - 89s 277ms/step - loss: 0.9022\n",
      "Epoch 25/50\n",
      "322/322 [==============================] - 89s 275ms/step - loss: 0.8673\n",
      "Epoch 26/50\n",
      "322/322 [==============================] - 89s 276ms/step - loss: 0.8532\n",
      "Epoch 27/50\n",
      "322/322 [==============================] - 89s 276ms/step - loss: 0.8124\n",
      "Epoch 28/50\n",
      "322/322 [==============================] - 89s 277ms/step - loss: 0.7813\n",
      "Epoch 29/50\n",
      "322/322 [==============================] - 89s 277ms/step - loss: 0.7876\n",
      "Epoch 30/50\n",
      "322/322 [==============================] - 89s 275ms/step - loss: 0.7537\n",
      "Epoch 31/50\n",
      "322/322 [==============================] - 89s 278ms/step - loss: 0.7491\n",
      "Epoch 32/50\n",
      "322/322 [==============================] - 89s 278ms/step - loss: 0.7147\n",
      "Epoch 33/50\n",
      "322/322 [==============================] - 89s 276ms/step - loss: 0.6908\n",
      "Epoch 34/50\n",
      "322/322 [==============================] - 89s 278ms/step - loss: 0.7233\n",
      "Epoch 35/50\n",
      "322/322 [==============================] - 90s 279ms/step - loss: 0.6832\n",
      "Epoch 36/50\n",
      "322/322 [==============================] - 90s 279ms/step - loss: 0.6442\n",
      "Epoch 37/50\n",
      "322/322 [==============================] - 89s 277ms/step - loss: 0.6717\n",
      "Epoch 38/50\n",
      "322/322 [==============================] - 90s 279ms/step - loss: 0.6884\n",
      "Epoch 39/50\n",
      "322/322 [==============================] - 89s 278ms/step - loss: 0.6402\n",
      "Epoch 40/50\n",
      "322/322 [==============================] - 89s 277ms/step - loss: 0.6562\n",
      "Epoch 41/50\n",
      "322/322 [==============================] - 90s 278ms/step - loss: 0.6235\n",
      "Epoch 42/50\n",
      "322/322 [==============================] - 91s 283ms/step - loss: 0.6251\n",
      "Epoch 43/50\n",
      "322/322 [==============================] - 90s 278ms/step - loss: 0.5575\n",
      "Epoch 44/50\n",
      "322/322 [==============================] - 89s 277ms/step - loss: 0.5534\n",
      "Epoch 45/50\n",
      "322/322 [==============================] - 90s 278ms/step - loss: 0.5828\n",
      "Epoch 46/50\n",
      "322/322 [==============================] - 89s 277ms/step - loss: 0.5724\n",
      "Epoch 47/50\n",
      "322/322 [==============================] - 89s 277ms/step - loss: 0.6351\n",
      "Epoch 48/50\n",
      "322/322 [==============================] - 89s 276ms/step - loss: 0.4900\n",
      "Epoch 49/50\n",
      "322/322 [==============================] - 90s 278ms/step - loss: 0.4952\n",
      "Epoch 50/50\n",
      "322/322 [==============================] - 89s 276ms/step - loss: 0.5250\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(45)\n",
    "tf.random.set_seed(45)\n",
    "\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dense, Activation, Dropout\n",
    "\n",
    "# Create a model\n",
    "model = Sequential()\n",
    "model.add(LSTM(256, input_shape = (window , 1)))\n",
    "model.add(Dropout(0.3))\n",
    "model.add(Dense(256))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Dense(126))\n",
    "model.add(Dense(number_of_output))\n",
    "model.add(Activation('softmax'))\n",
    "\n",
    "model.summary()\n",
    "\n",
    "model.compile(loss = \"categorical_crossentropy\", optimizer = \"adam\")\n",
    "\n",
    "history = model.fit(x_train_lstm, train_y_c,\n",
    "                    epochs = 50, batch_size = 32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "JdJ7v_1KBzY1",
    "outputId": "1a966527-78db-49fd-dd3d-bf768c9d1736"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_7\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm (LSTM)                  (None, 256)               264192    \n",
      "_________________________________________________________________\n",
      "dense_12 (Dense)             (None, 256)               65792     \n",
      "_________________________________________________________________\n",
      "dropout_6 (Dropout)          (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_13 (Dense)             (None, 128)               32896     \n",
      "_________________________________________________________________\n",
      "dense_14 (Dense)             (None, 128)               16512     \n",
      "_________________________________________________________________\n",
      "dropout_7 (Dropout)          (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_15 (Dense)             (None, 64)                8256      \n",
      "_________________________________________________________________\n",
      "dense_16 (Dense)             (None, 96)                6240      \n",
      "=================================================================\n",
      "Total params: 393,888\n",
      "Trainable params: 129,696\n",
      "Non-trainable params: 264,192\n",
      "_________________________________________________________________\n",
      "Epoch 1/100\n",
      "92/92 [==============================] - 9s 83ms/step - loss: 4.1828 - accuracy: 0.0920\n",
      "Epoch 2/100\n",
      "92/92 [==============================] - 8s 83ms/step - loss: 3.1240 - accuracy: 0.2566\n",
      "Epoch 3/100\n",
      "92/92 [==============================] - 8s 84ms/step - loss: 2.7425 - accuracy: 0.3257\n",
      "Epoch 4/100\n",
      "92/92 [==============================] - 8s 83ms/step - loss: 2.4714 - accuracy: 0.3724\n",
      "Epoch 5/100\n",
      "92/92 [==============================] - 8s 85ms/step - loss: 2.3414 - accuracy: 0.3658\n",
      "Epoch 6/100\n",
      "92/92 [==============================] - 8s 85ms/step - loss: 2.2251 - accuracy: 0.4182\n",
      "Epoch 7/100\n",
      "92/92 [==============================] - 8s 83ms/step - loss: 2.0914 - accuracy: 0.4366\n",
      "Epoch 8/100\n",
      "92/92 [==============================] - 8s 91ms/step - loss: 1.9456 - accuracy: 0.4849\n",
      "Epoch 9/100\n",
      "92/92 [==============================] - 8s 89ms/step - loss: 1.9049 - accuracy: 0.4781\n",
      "Epoch 10/100\n",
      "92/92 [==============================] - 8s 85ms/step - loss: 1.8231 - accuracy: 0.5006\n",
      "Epoch 11/100\n",
      "92/92 [==============================] - 8s 84ms/step - loss: 1.8320 - accuracy: 0.4891\n",
      "Epoch 12/100\n",
      "92/92 [==============================] - 8s 84ms/step - loss: 1.7281 - accuracy: 0.5162\n",
      "Epoch 13/100\n",
      "92/92 [==============================] - 8s 84ms/step - loss: 1.7072 - accuracy: 0.5215\n",
      "Epoch 14/100\n",
      "92/92 [==============================] - 8s 84ms/step - loss: 1.6234 - accuracy: 0.5391\n",
      "Epoch 15/100\n",
      "92/92 [==============================] - 8s 92ms/step - loss: 1.6704 - accuracy: 0.5417\n",
      "Epoch 16/100\n",
      "92/92 [==============================] - 8s 87ms/step - loss: 1.4932 - accuracy: 0.5769\n",
      "Epoch 17/100\n",
      "92/92 [==============================] - 8s 85ms/step - loss: 1.5524 - accuracy: 0.5541\n",
      "Epoch 18/100\n",
      "92/92 [==============================] - 8s 85ms/step - loss: 1.5284 - accuracy: 0.5745\n",
      "Epoch 19/100\n",
      "92/92 [==============================] - 8s 84ms/step - loss: 1.5204 - accuracy: 0.5653\n",
      "Epoch 20/100\n",
      "92/92 [==============================] - 8s 85ms/step - loss: 1.4122 - accuracy: 0.5900\n",
      "Epoch 21/100\n",
      "92/92 [==============================] - 8s 84ms/step - loss: 1.4400 - accuracy: 0.5742\n",
      "Epoch 22/100\n",
      "92/92 [==============================] - 8s 86ms/step - loss: 1.4105 - accuracy: 0.5985\n",
      "Epoch 23/100\n",
      "92/92 [==============================] - 8s 85ms/step - loss: 1.3808 - accuracy: 0.5946\n",
      "Epoch 24/100\n",
      "92/92 [==============================] - 8s 85ms/step - loss: 1.2691 - accuracy: 0.6247\n",
      "Epoch 25/100\n",
      "92/92 [==============================] - 8s 85ms/step - loss: 1.3694 - accuracy: 0.6158\n",
      "Epoch 26/100\n",
      "92/92 [==============================] - 8s 85ms/step - loss: 1.2977 - accuracy: 0.6310\n",
      "Epoch 27/100\n",
      "92/92 [==============================] - 8s 86ms/step - loss: 1.3410 - accuracy: 0.6033\n",
      "Epoch 28/100\n",
      "92/92 [==============================] - 8s 87ms/step - loss: 1.2165 - accuracy: 0.6502\n",
      "Epoch 29/100\n",
      "92/92 [==============================] - 8s 86ms/step - loss: 1.3024 - accuracy: 0.6240\n",
      "Epoch 30/100\n",
      "92/92 [==============================] - 8s 85ms/step - loss: 1.2762 - accuracy: 0.6197\n",
      "Epoch 31/100\n",
      "92/92 [==============================] - 8s 87ms/step - loss: 1.2596 - accuracy: 0.6434\n",
      "Epoch 32/100\n",
      "92/92 [==============================] - 8s 86ms/step - loss: 1.2595 - accuracy: 0.6330\n",
      "Epoch 33/100\n",
      "92/92 [==============================] - 8s 85ms/step - loss: 1.1795 - accuracy: 0.6621\n",
      "Epoch 34/100\n",
      "92/92 [==============================] - 8s 86ms/step - loss: 1.2280 - accuracy: 0.6388\n",
      "Epoch 35/100\n",
      "92/92 [==============================] - 8s 86ms/step - loss: 1.1719 - accuracy: 0.6539\n",
      "Epoch 36/100\n",
      "92/92 [==============================] - 8s 87ms/step - loss: 1.1456 - accuracy: 0.6672\n",
      "Epoch 37/100\n",
      "92/92 [==============================] - 8s 86ms/step - loss: 1.1434 - accuracy: 0.6726\n",
      "Epoch 38/100\n",
      "92/92 [==============================] - 8s 86ms/step - loss: 1.0968 - accuracy: 0.6799\n",
      "Epoch 39/100\n",
      "92/92 [==============================] - 8s 86ms/step - loss: 1.1774 - accuracy: 0.6585\n",
      "Epoch 40/100\n",
      "92/92 [==============================] - 8s 85ms/step - loss: 1.1360 - accuracy: 0.6706\n",
      "Epoch 41/100\n",
      "92/92 [==============================] - 8s 87ms/step - loss: 1.1401 - accuracy: 0.6599\n",
      "Epoch 42/100\n",
      "92/92 [==============================] - 8s 87ms/step - loss: 1.1888 - accuracy: 0.6549\n",
      "Epoch 43/100\n",
      "92/92 [==============================] - 8s 86ms/step - loss: 1.1178 - accuracy: 0.6643\n",
      "Epoch 44/100\n",
      "92/92 [==============================] - 8s 88ms/step - loss: 1.1422 - accuracy: 0.6717\n",
      "Epoch 45/100\n",
      "92/92 [==============================] - 8s 89ms/step - loss: 1.0988 - accuracy: 0.6797\n",
      "Epoch 46/100\n",
      "92/92 [==============================] - 8s 88ms/step - loss: 1.0651 - accuracy: 0.6799\n",
      "Epoch 47/100\n",
      "92/92 [==============================] - 9s 96ms/step - loss: 1.0649 - accuracy: 0.6917\n",
      "Epoch 48/100\n",
      "92/92 [==============================] - 9s 96ms/step - loss: 1.0980 - accuracy: 0.6666\n",
      "Epoch 49/100\n",
      "92/92 [==============================] - 8s 87ms/step - loss: 1.0362 - accuracy: 0.6960\n",
      "Epoch 50/100\n",
      "92/92 [==============================] - 8s 86ms/step - loss: 1.0062 - accuracy: 0.7038\n",
      "Epoch 51/100\n",
      "92/92 [==============================] - 8s 86ms/step - loss: 1.0615 - accuracy: 0.6881\n",
      "Epoch 52/100\n",
      "92/92 [==============================] - 8s 86ms/step - loss: 1.0694 - accuracy: 0.6912\n",
      "Epoch 53/100\n",
      "92/92 [==============================] - 8s 89ms/step - loss: 1.0039 - accuracy: 0.7139\n",
      "Epoch 54/100\n",
      "92/92 [==============================] - 9s 94ms/step - loss: 0.9883 - accuracy: 0.7090\n",
      "Epoch 55/100\n",
      "92/92 [==============================] - 8s 86ms/step - loss: 0.9712 - accuracy: 0.7134\n",
      "Epoch 56/100\n",
      "92/92 [==============================] - 8s 88ms/step - loss: 0.9968 - accuracy: 0.7074\n",
      "Epoch 57/100\n",
      "92/92 [==============================] - 8s 87ms/step - loss: 0.9599 - accuracy: 0.7216\n",
      "Epoch 58/100\n",
      "92/92 [==============================] - 8s 85ms/step - loss: 0.9593 - accuracy: 0.7098\n",
      "Epoch 59/100\n",
      "92/92 [==============================] - 8s 84ms/step - loss: 0.9664 - accuracy: 0.7160\n",
      "Epoch 60/100\n",
      "92/92 [==============================] - 8s 86ms/step - loss: 0.9776 - accuracy: 0.7132\n",
      "Epoch 61/100\n",
      "92/92 [==============================] - 8s 86ms/step - loss: 0.9526 - accuracy: 0.7236\n",
      "Epoch 62/100\n",
      "92/92 [==============================] - 8s 84ms/step - loss: 0.9396 - accuracy: 0.7161\n",
      "Epoch 63/100\n",
      "92/92 [==============================] - 8s 85ms/step - loss: 0.9755 - accuracy: 0.7180\n",
      "Epoch 64/100\n",
      "92/92 [==============================] - 8s 85ms/step - loss: 0.9945 - accuracy: 0.7068\n",
      "Epoch 65/100\n",
      "92/92 [==============================] - 8s 83ms/step - loss: 0.9289 - accuracy: 0.7250\n",
      "Epoch 66/100\n",
      "92/92 [==============================] - 8s 84ms/step - loss: 0.9463 - accuracy: 0.6962\n",
      "Epoch 67/100\n",
      "92/92 [==============================] - 8s 84ms/step - loss: 0.9193 - accuracy: 0.7239\n",
      "Epoch 68/100\n",
      "92/92 [==============================] - 8s 86ms/step - loss: 0.9470 - accuracy: 0.7122\n",
      "Epoch 69/100\n",
      "92/92 [==============================] - 8s 86ms/step - loss: 0.9306 - accuracy: 0.7163\n",
      "Epoch 70/100\n",
      "92/92 [==============================] - 8s 86ms/step - loss: 0.9279 - accuracy: 0.7211\n",
      "Epoch 71/100\n",
      "92/92 [==============================] - 8s 84ms/step - loss: 0.8782 - accuracy: 0.7345\n",
      "Epoch 72/100\n",
      "92/92 [==============================] - 8s 84ms/step - loss: 0.9028 - accuracy: 0.7216\n",
      "Epoch 73/100\n",
      "92/92 [==============================] - 8s 84ms/step - loss: 0.9345 - accuracy: 0.7113\n",
      "Epoch 74/100\n",
      "92/92 [==============================] - 8s 85ms/step - loss: 0.9756 - accuracy: 0.7066\n",
      "Epoch 75/100\n",
      "92/92 [==============================] - 8s 85ms/step - loss: 0.8845 - accuracy: 0.7356\n",
      "Epoch 76/100\n",
      "92/92 [==============================] - 8s 85ms/step - loss: 0.8836 - accuracy: 0.7367\n",
      "Epoch 77/100\n",
      "92/92 [==============================] - 8s 85ms/step - loss: 0.9207 - accuracy: 0.7131\n",
      "Epoch 78/100\n",
      "92/92 [==============================] - 8s 83ms/step - loss: 0.8738 - accuracy: 0.7424\n",
      "Epoch 79/100\n",
      "92/92 [==============================] - 8s 85ms/step - loss: 0.9184 - accuracy: 0.7197\n",
      "Epoch 80/100\n",
      "92/92 [==============================] - 8s 84ms/step - loss: 0.8467 - accuracy: 0.7347\n",
      "Epoch 81/100\n",
      "92/92 [==============================] - 8s 83ms/step - loss: 0.8535 - accuracy: 0.7295\n",
      "Epoch 82/100\n",
      "92/92 [==============================] - 8s 83ms/step - loss: 0.9022 - accuracy: 0.7250\n",
      "Epoch 83/100\n",
      "92/92 [==============================] - 8s 84ms/step - loss: 0.8845 - accuracy: 0.7320\n",
      "Epoch 84/100\n",
      "92/92 [==============================] - 8s 84ms/step - loss: 0.8625 - accuracy: 0.7396\n",
      "Epoch 85/100\n",
      "92/92 [==============================] - 8s 85ms/step - loss: 0.8267 - accuracy: 0.7493\n",
      "Epoch 86/100\n",
      "92/92 [==============================] - 8s 86ms/step - loss: 0.8162 - accuracy: 0.7418\n",
      "Epoch 87/100\n",
      "92/92 [==============================] - 9s 94ms/step - loss: 0.9109 - accuracy: 0.7237\n",
      "Epoch 88/100\n",
      "92/92 [==============================] - 8s 85ms/step - loss: 0.8796 - accuracy: 0.7395\n",
      "Epoch 89/100\n",
      "92/92 [==============================] - 8s 84ms/step - loss: 0.8661 - accuracy: 0.7405\n",
      "Epoch 90/100\n",
      "92/92 [==============================] - 8s 86ms/step - loss: 0.7886 - accuracy: 0.7580\n",
      "Epoch 91/100\n",
      "92/92 [==============================] - 8s 85ms/step - loss: 0.8237 - accuracy: 0.7563\n",
      "Epoch 92/100\n",
      "92/92 [==============================] - 8s 87ms/step - loss: 0.8470 - accuracy: 0.7425\n",
      "Epoch 93/100\n",
      "92/92 [==============================] - 8s 89ms/step - loss: 0.8153 - accuracy: 0.7542\n",
      "Epoch 94/100\n",
      "92/92 [==============================] - 8s 85ms/step - loss: 0.8390 - accuracy: 0.7439\n",
      "Epoch 95/100\n",
      "92/92 [==============================] - 8s 84ms/step - loss: 0.8364 - accuracy: 0.7447\n",
      "Epoch 96/100\n",
      "92/92 [==============================] - 8s 85ms/step - loss: 0.8379 - accuracy: 0.7319\n",
      "Epoch 97/100\n",
      "92/92 [==============================] - 8s 84ms/step - loss: 0.8341 - accuracy: 0.7460\n",
      "Epoch 98/100\n",
      "92/92 [==============================] - 8s 84ms/step - loss: 0.9206 - accuracy: 0.7164\n",
      "Epoch 99/100\n",
      "92/92 [==============================] - 8s 84ms/step - loss: 0.8604 - accuracy: 0.7320\n",
      "Epoch 100/100\n",
      "92/92 [==============================] - 8s 87ms/step - loss: 0.8827 - accuracy: 0.7282\n"
     ]
    }
   ],
   "source": [
    "# Create a Sequential model.\n",
    "model1 = Sequential()\n",
    "\n",
    "# Load the extracted features from model3 (remove the classification layers).\n",
    "for layer in model.layers[:-6]:\n",
    "  layer.trainable = False # DO NOT train these layers.\n",
    "  model1.add(layer)\n",
    "\n",
    "# Initialize to random weights the classification layers and train them.\n",
    "model1.add(Dense(256))\n",
    "model1.add(Dropout(0.2))\n",
    "model1.add(Dense(128))\n",
    "model1.add(Dense(128))\n",
    "model1.add(Dropout(0.25))\n",
    "model1.add(Dense(64))\n",
    "model1.add(Dense(number_of_output1, activation = 'softmax'))\n",
    "\n",
    "# Print the model.\n",
    "model1.summary()\n",
    "\n",
    "# Compile the model.\n",
    "model1.compile(loss = \"categorical_crossentropy\", optimizer = \"adam\", metrics = [\"accuracy\"])\n",
    "\n",
    "# Fit the model.\n",
    "history1 = model1.fit(x_train_lstm1, train_y_c1,\n",
    "                    epochs = 100, batch_size = 32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "kyMnNhymdgjE"
   },
   "outputs": [],
   "source": [
    "import random as rn\n",
    "\n",
    "#starting with a random array of 90 notes\n",
    "\n",
    "start = rn.randint(0, len(train1)-window)\n",
    "int_to_note = dict((number, note) for number, note in enumerate(pitchnames1))\n",
    "pattern = train1[ start : start + window]\n",
    "prediction_output = []\n",
    "\n",
    "# generate 500 notes\n",
    "for note_index in range(500):\n",
    "    prediction_input = np.reshape(pattern, (-1, window, 1))\n",
    "    prediction = model1.predict(prediction_input)\n",
    "    index = np.argmax(prediction)\n",
    "    result = int_to_note[index]\n",
    "    prediction_output.append(result)\n",
    "    pattern = np.append(pattern, index)\n",
    "    pattern = pattern[1:len(pattern)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Exd7JMvr7N9a"
   },
   "outputs": [],
   "source": [
    "offset = 0\n",
    "output_notes = []\n",
    "\n",
    "# create note and chord objects based on the values generated by the model\n",
    "for pattern in prediction_output:\n",
    "    # pattern is a chord\n",
    "    if ('.' in pattern) or pattern.isdigit():\n",
    "        notes_in_chord = pattern.split('.')\n",
    "        notes = []\n",
    "        for current_note in notes_in_chord:\n",
    "            new_note = note.Note(int(current_note))\n",
    "            new_note.storedInstrument = instrument.Piano()\n",
    "            notes.append(new_note)\n",
    "        new_chord = chord.Chord(notes)\n",
    "        new_chord.offset = offset\n",
    "        output_notes.append(new_chord)\n",
    "    # pattern is a note\n",
    "    else:\n",
    "        new_note = note.Note(pattern)\n",
    "        new_note.offset = offset\n",
    "        new_note.storedInstrument = instrument.Piano()\n",
    "        output_notes.append(new_note)\n",
    "    # increase offset each iteration so that notes do not stack\n",
    "    offset += 0.4 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "id": "1fTfutYgyCeD",
    "outputId": "e5051b9d-df09-4633-80a9-36e5d22dbca4"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.google.colaboratory.intrinsic+json": {
       "type": "string"
      },
      "text/plain": [
       "'/content/drive/MyDrive/music_train.mid'"
      ]
     },
     "execution_count": 37,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from music21 import stream\n",
    "#creating the music file\n",
    "final_product = stream.Stream(output_notes)\n",
    "final_product.write('midi', fp='/content/drive/MyDrive/music_train.mid')"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "music_project.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
